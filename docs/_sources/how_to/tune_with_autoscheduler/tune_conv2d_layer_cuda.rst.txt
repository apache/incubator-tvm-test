
.. DO NOT EDIT. THIS FILE WAS AUTOMATICALLY GENERATED BY
.. TVM'S MONKEY-PATCHED VERSION OF SPHINX-GALLERY. TO MAKE
.. CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "how_to/tune_with_autoscheduler/tune_conv2d_layer_cuda.py"

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        This tutorial can be used interactively with Google Colab! You can also click
        :ref:`here <sphx_glr_download_how_to_tune_with_autoscheduler_tune_conv2d_layer_cuda.py>` to run the Jupyter notebook locally.

        .. image:: https://raw.githubusercontent.com/tlc-pack/web-data/main/images/utilities/colab_button.svg
            :align: center
            :target: https://colab.research.google.com/github/apache/tvm-site/blob/asf-site/docs/_downloads/5f1f7bd7d90710fd404f7bcdc4965622/tune_conv2d_layer_cuda.ipynb
            :width: 300px

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_how_to_tune_with_autoscheduler_tune_conv2d_layer_cuda.py:


.. _auto-scheduler-conv-gpu:

Auto-scheduling a Convolution Layer for GPU
===========================================
**Author**: `Lianmin Zheng <https://github.com/merrymercy>`_,             `Chengfan Jia <https://github.com/jcf94/>`_

This is a tutorial on how to use the auto-scheduler for GPUs.

Different from the template-based :ref:`autotvm <tutorials-autotvm-sec>` which relies on
manual templates to define the search space, the auto-scheduler does not require any templates.
Users only need to write the computation declaration without any schedule commands or templates.
The auto-scheduler can automatically generate a large search space and
find a good schedule in the space.

We use a convolution layer as an example in this tutorial.

Note that this tutorial will not run on Windows or recent versions of macOS. To
get it to run, you will need to wrap the body of this tutorial in a :code:`if
__name__ == "__main__":` block.

.. GENERATED FROM PYTHON SOURCE LINES 39-47

.. code-block:: default


    import os

    import numpy as np
    import tvm
    from tvm import te, auto_scheduler, topi
    from tvm.topi.testing import conv2d_nchw_python








.. GENERATED FROM PYTHON SOURCE LINES 51-56

Define the computation
^^^^^^^^^^^^^^^^^^^^^^
To begin with, let us define the computation of a convolution layer.
The function should return the list of input/output tensors.
From these tensors, the auto-scheduler can get the whole computational graph.

.. GENERATED FROM PYTHON SOURCE LINES 56-68

.. code-block:: default



    @auto_scheduler.register_workload
    def conv2d_layer(N, H, W, CO, CI, KH, KW, stride, padding):
        data = te.placeholder((N, CI, H, W), name="data")
        kernel = te.placeholder((CO, CI, KH, KW), name="kernel")
        bias = te.placeholder((1, CO, 1, 1), name="bias")
        conv = topi.nn.conv2d_nchw(data, kernel, stride, padding, dilation=1, out_dtype="float32")
        out = topi.nn.relu(conv + bias)
        return [data, kernel, bias, out]









.. GENERATED FROM PYTHON SOURCE LINES 69-72

Create the search task
^^^^^^^^^^^^^^^^^^^^^^
We then create a search task for the last convolution layer in the resnet.

.. GENERATED FROM PYTHON SOURCE LINES 72-85

.. code-block:: default


    target = tvm.target.Target("cuda")

    # Use the last layer in ResNet-50
    N, H, W, CO, CI, KH, KW, strides, padding = 1, 7, 7, 512, 512, 3, 3, (1, 1), (1, 1)
    task = auto_scheduler.SearchTask(
        func=conv2d_layer, args=(N, H, W, CO, CI, KH, KW, strides, padding), target=target
    )

    # Inspect the computational graph
    print("Computational DAG:")
    print(task.compute_dag)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Computational DAG:
    data = PLACEHOLDER [1, 512, 7, 7]
    pad_temp(i0, i1, i2, i3) = tir.if_then_else(((((i2 >= 1) && (i2 < 8)) && (i3 >= 1)) && (i3 < 8)), data[i0, i1, (i2 - 1), (i3 - 1)], 0f)
    kernel = PLACEHOLDER [512, 512, 3, 3]
    conv2d_nchw(nn, ff, yy, xx) += (pad_temp[nn, rc, (yy + ry), (xx + rx)]*kernel[ff, rc, ry, rx])
    bias = PLACEHOLDER [1, 512, 1, 1]
    T_add(ax0, ax1, ax2, ax3) = (conv2d_nchw[ax0, ax1, ax2, ax3] + bias[ax0, ax1, 0, 0])
    compute(i0, i1, i2, i3) = max(T_add[i0, i1, i2, i3], 0f)





.. GENERATED FROM PYTHON SOURCE LINES 86-103

Next, we set parameters for the auto-scheduler. These parameters
mainly specify how we do the measurement during the search.

* :code:`measure_ctx` launches a different process for measurement to
  provide isolation. It can protect the main process from GPU crashes
  during measurement and avoid other runtime conflicts.
* :code:`min_repeat_ms` defines the minimum duration of one "repeat" in every measurement.
  This can warmup the GPU, which is necessary to get accurate measurement results.
  Typically, we recommend a value >= 300 ms.
* :code:`num_measure_trials` is the number of measurement trials we can use during the search.
  We only make 10 trials in this tutorial for a fast demonstration. In practice, 1000 is a
  good value for the search to converge. You can do more trials according to your time budget.
* In addition, we use :code:`RecordToFile` to dump measurement records into a file `conv2d.json`.
  The measurement records can be used to query the history best, resume the search,
  and do more analyses later.
* see :any:`auto_scheduler.TuningOptions`,
  :any:`auto_scheduler.LocalRPCMeasureContext` for more parameters.

.. GENERATED FROM PYTHON SOURCE LINES 103-113

.. code-block:: default


    log_file = "conv2d.json"
    measure_ctx = auto_scheduler.LocalRPCMeasureContext(min_repeat_ms=300)
    tune_option = auto_scheduler.TuningOptions(
        num_measure_trials=10,  # change this to 1000 to achieve the best performance
        runner=measure_ctx.runner,
        measure_callbacks=[auto_scheduler.RecordToFile(log_file)],
        verbose=2,
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Get devices for measurement successfully!




.. GENERATED FROM PYTHON SOURCE LINES 114-120

Run the search
^^^^^^^^^^^^^^
Now we get all inputs ready. Pretty simple, isn't it?
We can kick off the search and let the auto-scheduler do its magic.
After some measurement trials, we can load the best schedule from the log
file and apply it.

.. GENERATED FROM PYTHON SOURCE LINES 120-129

.. code-block:: default


    # Run auto-tuning (search)
    task.tune(tune_option)
    # Apply the best schedule
    sch, args = task.apply_best(log_file)

    # Kill the measurement process
    del measure_ctx








.. GENERATED FROM PYTHON SOURCE LINES 130-133

We can lower the schedule to see the IR after auto-scheduling.
The auto-scheduler correctly performs optimizations including multi-level tiling,
cooperative fetching, unrolling and operator fusion.

.. GENERATED FROM PYTHON SOURCE LINES 133-137

.. code-block:: default


    print("Lowered TIR:")
    print(tvm.lower(sch, args, simple_mode=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Lowered TIR:
    # from tvm.script import ir as I
    # from tvm.script import tir as T

    @I.ir_module
    class Module:
        @T.prim_func
        def main(data: T.Buffer((1, 512, 7, 7), "float32"), kernel: T.Buffer((512, 512, 3, 3), "float32"), bias: T.Buffer((1, 512, 1, 1), "float32"), compute: T.Buffer((1, 512, 7, 7), "float32")):
            T.func_attr({"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True})
            blockIdx_x = T.env_thread("blockIdx.x")
            T.launch_thread(blockIdx_x, 16)
            conv2d_nchw = T.allocate([2], "float32", "local")
            pad_temp_shared = T.allocate([648], "float32", "shared")
            kernel_shared = T.allocate([2304], "float32", "shared")
            threadIdx_x = T.env_thread("threadIdx.x")
            T.launch_thread(threadIdx_x, 784)
            conv2d_nchw_1 = T.Buffer((2,), data=conv2d_nchw, scope="local", align=8)
            conv2d_nchw_1[0] = T.float32(0)
            conv2d_nchw_1[1] = T.float32(0)
            for rc_outer_outer in range(64):
                cse_var_1: T.int32 = rc_outer_outer * 72
                threadIdx_x_1 = T.env_thread("threadIdx.x")
                pad_temp_shared_1 = T.Buffer((648,), data=pad_temp_shared, scope="shared")
                with T.launch_thread(threadIdx_x_1, 784):
                    if T.likely(threadIdx_x_1 < 648):
                        data_1 = T.Buffer((25088,), data=data.data)
                        pad_temp_shared_1[threadIdx_x_1] = T.if_then_else(9 <= threadIdx_x_1 % 81 and threadIdx_x_1 % 81 < 72 and 1 <= threadIdx_x_1 % 9 and threadIdx_x_1 % 9 < 8, data_1[rc_outer_outer * 392 + threadIdx_x_1 // 81 * 49 + threadIdx_x_1 % 81 // 9 * 7 + threadIdx_x_1 % 9 - 8], T.float32(0))
                threadIdx_x_2 = T.env_thread("threadIdx.x")
                kernel_shared_1 = T.Buffer((2304,), data=kernel_shared, scope="shared")
                kernel_1 = T.Buffer((2359296,), data=kernel.data)
                with T.launch_thread(threadIdx_x_2, 784):
                    kernel_shared_1[threadIdx_x_2] = kernel_1[blockIdx_x * 147456 + threadIdx_x_2 // 72 * 4608 + cse_var_1 + threadIdx_x_2 % 72]
                with T.launch_thread(threadIdx_x_2, 784):
                    kernel_shared_1[threadIdx_x_2 + 784] = kernel_1[blockIdx_x * 147456 + (threadIdx_x_2 + 784) // 72 * 4608 + cse_var_1 + (threadIdx_x_2 + 64) % 72 // 3 * 3 + (threadIdx_x_2 + 1) % 3]
                with T.launch_thread(threadIdx_x_2, 784):
                    if T.likely(threadIdx_x_2 < 736):
                        kernel_shared_1[threadIdx_x_2 + 1568] = kernel_1[blockIdx_x * 147456 + (threadIdx_x_2 + 1568) // 72 * 4608 + cse_var_1 + (threadIdx_x_2 + 56) % 72 // 3 * 3 + (threadIdx_x_2 + 2) % 3]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7] * kernel_shared_1[threadIdx_x // 49 * 144]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7] * kernel_shared_1[threadIdx_x // 49 * 144 + 72]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 1] * kernel_shared_1[threadIdx_x // 49 * 144 + 1]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 1] * kernel_shared_1[threadIdx_x // 49 * 144 + 73]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 2] * kernel_shared_1[threadIdx_x // 49 * 144 + 2]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 2] * kernel_shared_1[threadIdx_x // 49 * 144 + 74]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 81] * kernel_shared_1[threadIdx_x // 49 * 144 + 9]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 81] * kernel_shared_1[threadIdx_x // 49 * 144 + 81]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 82] * kernel_shared_1[threadIdx_x // 49 * 144 + 10]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 82] * kernel_shared_1[threadIdx_x // 49 * 144 + 82]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 83] * kernel_shared_1[threadIdx_x // 49 * 144 + 11]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 83] * kernel_shared_1[threadIdx_x // 49 * 144 + 83]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 162] * kernel_shared_1[threadIdx_x // 49 * 144 + 18]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 162] * kernel_shared_1[threadIdx_x // 49 * 144 + 90]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 163] * kernel_shared_1[threadIdx_x // 49 * 144 + 19]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 163] * kernel_shared_1[threadIdx_x // 49 * 144 + 91]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 164] * kernel_shared_1[threadIdx_x // 49 * 144 + 20]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 164] * kernel_shared_1[threadIdx_x // 49 * 144 + 92]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 243] * kernel_shared_1[threadIdx_x // 49 * 144 + 27]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 243] * kernel_shared_1[threadIdx_x // 49 * 144 + 99]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 244] * kernel_shared_1[threadIdx_x // 49 * 144 + 28]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 244] * kernel_shared_1[threadIdx_x // 49 * 144 + 100]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 245] * kernel_shared_1[threadIdx_x // 49 * 144 + 29]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 245] * kernel_shared_1[threadIdx_x // 49 * 144 + 101]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 324] * kernel_shared_1[threadIdx_x // 49 * 144 + 36]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 324] * kernel_shared_1[threadIdx_x // 49 * 144 + 108]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 325] * kernel_shared_1[threadIdx_x // 49 * 144 + 37]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 325] * kernel_shared_1[threadIdx_x // 49 * 144 + 109]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 326] * kernel_shared_1[threadIdx_x // 49 * 144 + 38]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 326] * kernel_shared_1[threadIdx_x // 49 * 144 + 110]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 405] * kernel_shared_1[threadIdx_x // 49 * 144 + 45]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 405] * kernel_shared_1[threadIdx_x // 49 * 144 + 117]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 406] * kernel_shared_1[threadIdx_x // 49 * 144 + 46]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 406] * kernel_shared_1[threadIdx_x // 49 * 144 + 118]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 407] * kernel_shared_1[threadIdx_x // 49 * 144 + 47]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 407] * kernel_shared_1[threadIdx_x // 49 * 144 + 119]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 486] * kernel_shared_1[threadIdx_x // 49 * 144 + 54]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 486] * kernel_shared_1[threadIdx_x // 49 * 144 + 126]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 487] * kernel_shared_1[threadIdx_x // 49 * 144 + 55]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 487] * kernel_shared_1[threadIdx_x // 49 * 144 + 127]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 488] * kernel_shared_1[threadIdx_x // 49 * 144 + 56]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 488] * kernel_shared_1[threadIdx_x // 49 * 144 + 128]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 567] * kernel_shared_1[threadIdx_x // 49 * 144 + 63]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 567] * kernel_shared_1[threadIdx_x // 49 * 144 + 135]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 568] * kernel_shared_1[threadIdx_x // 49 * 144 + 64]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 568] * kernel_shared_1[threadIdx_x // 49 * 144 + 136]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 569] * kernel_shared_1[threadIdx_x // 49 * 144 + 65]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 569] * kernel_shared_1[threadIdx_x // 49 * 144 + 137]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 9] * kernel_shared_1[threadIdx_x // 49 * 144 + 3]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 9] * kernel_shared_1[threadIdx_x // 49 * 144 + 75]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 10] * kernel_shared_1[threadIdx_x // 49 * 144 + 4]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 10] * kernel_shared_1[threadIdx_x // 49 * 144 + 76]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 11] * kernel_shared_1[threadIdx_x // 49 * 144 + 5]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 11] * kernel_shared_1[threadIdx_x // 49 * 144 + 77]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 90] * kernel_shared_1[threadIdx_x // 49 * 144 + 12]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 90] * kernel_shared_1[threadIdx_x // 49 * 144 + 84]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 91] * kernel_shared_1[threadIdx_x // 49 * 144 + 13]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 91] * kernel_shared_1[threadIdx_x // 49 * 144 + 85]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 92] * kernel_shared_1[threadIdx_x // 49 * 144 + 14]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 92] * kernel_shared_1[threadIdx_x // 49 * 144 + 86]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 171] * kernel_shared_1[threadIdx_x // 49 * 144 + 21]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 171] * kernel_shared_1[threadIdx_x // 49 * 144 + 93]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 172] * kernel_shared_1[threadIdx_x // 49 * 144 + 22]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 172] * kernel_shared_1[threadIdx_x // 49 * 144 + 94]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 173] * kernel_shared_1[threadIdx_x // 49 * 144 + 23]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 173] * kernel_shared_1[threadIdx_x // 49 * 144 + 95]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 252] * kernel_shared_1[threadIdx_x // 49 * 144 + 30]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 252] * kernel_shared_1[threadIdx_x // 49 * 144 + 102]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 253] * kernel_shared_1[threadIdx_x // 49 * 144 + 31]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 253] * kernel_shared_1[threadIdx_x // 49 * 144 + 103]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 254] * kernel_shared_1[threadIdx_x // 49 * 144 + 32]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 254] * kernel_shared_1[threadIdx_x // 49 * 144 + 104]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 333] * kernel_shared_1[threadIdx_x // 49 * 144 + 39]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 333] * kernel_shared_1[threadIdx_x // 49 * 144 + 111]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 334] * kernel_shared_1[threadIdx_x // 49 * 144 + 40]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 334] * kernel_shared_1[threadIdx_x // 49 * 144 + 112]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 335] * kernel_shared_1[threadIdx_x // 49 * 144 + 41]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 335] * kernel_shared_1[threadIdx_x // 49 * 144 + 113]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 414] * kernel_shared_1[threadIdx_x // 49 * 144 + 48]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 414] * kernel_shared_1[threadIdx_x // 49 * 144 + 120]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 415] * kernel_shared_1[threadIdx_x // 49 * 144 + 49]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 415] * kernel_shared_1[threadIdx_x // 49 * 144 + 121]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 416] * kernel_shared_1[threadIdx_x // 49 * 144 + 50]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 416] * kernel_shared_1[threadIdx_x // 49 * 144 + 122]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 495] * kernel_shared_1[threadIdx_x // 49 * 144 + 57]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 495] * kernel_shared_1[threadIdx_x // 49 * 144 + 129]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 496] * kernel_shared_1[threadIdx_x // 49 * 144 + 58]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 496] * kernel_shared_1[threadIdx_x // 49 * 144 + 130]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 497] * kernel_shared_1[threadIdx_x // 49 * 144 + 59]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 497] * kernel_shared_1[threadIdx_x // 49 * 144 + 131]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 576] * kernel_shared_1[threadIdx_x // 49 * 144 + 66]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 576] * kernel_shared_1[threadIdx_x // 49 * 144 + 138]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 577] * kernel_shared_1[threadIdx_x // 49 * 144 + 67]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 577] * kernel_shared_1[threadIdx_x // 49 * 144 + 139]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 578] * kernel_shared_1[threadIdx_x // 49 * 144 + 68]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 578] * kernel_shared_1[threadIdx_x // 49 * 144 + 140]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 18] * kernel_shared_1[threadIdx_x // 49 * 144 + 6]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 18] * kernel_shared_1[threadIdx_x // 49 * 144 + 78]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 19] * kernel_shared_1[threadIdx_x // 49 * 144 + 7]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 19] * kernel_shared_1[threadIdx_x // 49 * 144 + 79]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 20] * kernel_shared_1[threadIdx_x // 49 * 144 + 8]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 20] * kernel_shared_1[threadIdx_x // 49 * 144 + 80]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 99] * kernel_shared_1[threadIdx_x // 49 * 144 + 15]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 99] * kernel_shared_1[threadIdx_x // 49 * 144 + 87]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 100] * kernel_shared_1[threadIdx_x // 49 * 144 + 16]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 100] * kernel_shared_1[threadIdx_x // 49 * 144 + 88]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 101] * kernel_shared_1[threadIdx_x // 49 * 144 + 17]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 101] * kernel_shared_1[threadIdx_x // 49 * 144 + 89]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 180] * kernel_shared_1[threadIdx_x // 49 * 144 + 24]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 180] * kernel_shared_1[threadIdx_x // 49 * 144 + 96]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 181] * kernel_shared_1[threadIdx_x // 49 * 144 + 25]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 181] * kernel_shared_1[threadIdx_x // 49 * 144 + 97]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 182] * kernel_shared_1[threadIdx_x // 49 * 144 + 26]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 182] * kernel_shared_1[threadIdx_x // 49 * 144 + 98]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 261] * kernel_shared_1[threadIdx_x // 49 * 144 + 33]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 261] * kernel_shared_1[threadIdx_x // 49 * 144 + 105]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 262] * kernel_shared_1[threadIdx_x // 49 * 144 + 34]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 262] * kernel_shared_1[threadIdx_x // 49 * 144 + 106]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 263] * kernel_shared_1[threadIdx_x // 49 * 144 + 35]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 263] * kernel_shared_1[threadIdx_x // 49 * 144 + 107]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 342] * kernel_shared_1[threadIdx_x // 49 * 144 + 42]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 342] * kernel_shared_1[threadIdx_x // 49 * 144 + 114]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 343] * kernel_shared_1[threadIdx_x // 49 * 144 + 43]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 343] * kernel_shared_1[threadIdx_x // 49 * 144 + 115]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 344] * kernel_shared_1[threadIdx_x // 49 * 144 + 44]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 344] * kernel_shared_1[threadIdx_x // 49 * 144 + 116]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 423] * kernel_shared_1[threadIdx_x // 49 * 144 + 51]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 423] * kernel_shared_1[threadIdx_x // 49 * 144 + 123]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 424] * kernel_shared_1[threadIdx_x // 49 * 144 + 52]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 424] * kernel_shared_1[threadIdx_x // 49 * 144 + 124]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 425] * kernel_shared_1[threadIdx_x // 49 * 144 + 53]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 425] * kernel_shared_1[threadIdx_x // 49 * 144 + 125]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 504] * kernel_shared_1[threadIdx_x // 49 * 144 + 60]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 504] * kernel_shared_1[threadIdx_x // 49 * 144 + 132]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 505] * kernel_shared_1[threadIdx_x // 49 * 144 + 61]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 505] * kernel_shared_1[threadIdx_x // 49 * 144 + 133]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 506] * kernel_shared_1[threadIdx_x // 49 * 144 + 62]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 506] * kernel_shared_1[threadIdx_x // 49 * 144 + 134]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 585] * kernel_shared_1[threadIdx_x // 49 * 144 + 69]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 585] * kernel_shared_1[threadIdx_x // 49 * 144 + 141]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 586] * kernel_shared_1[threadIdx_x // 49 * 144 + 70]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 586] * kernel_shared_1[threadIdx_x // 49 * 144 + 142]
                conv2d_nchw_1[0] = conv2d_nchw_1[0] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 587] * kernel_shared_1[threadIdx_x // 49 * 144 + 71]
                conv2d_nchw_1[1] = conv2d_nchw_1[1] + pad_temp_shared_1[threadIdx_x % 49 // 7 * 9 + threadIdx_x % 7 + 587] * kernel_shared_1[threadIdx_x // 49 * 144 + 143]
            for i1_inner in range(2):
                compute_1 = T.Buffer((25088,), data=compute.data)
                bias_1 = T.Buffer((512,), data=bias.data)
                compute_1[blockIdx_x * 1568 + threadIdx_x // 49 * 98 + i1_inner * 49 + threadIdx_x % 49] = T.max(conv2d_nchw_1[i1_inner] + bias_1[blockIdx_x * 32 + threadIdx_x // 49 * 2 + i1_inner], T.float32(0))




.. GENERATED FROM PYTHON SOURCE LINES 138-141

Check correctness and evaluate performance
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
We build the binary and check its correctness and performance.

.. GENERATED FROM PYTHON SOURCE LINES 141-168

.. code-block:: default


    func = tvm.build(sch, args, target)

    # Check correctness
    data_np = np.random.uniform(size=(N, CI, H, W)).astype(np.float32)
    weight_np = np.random.uniform(size=(CO, CI, KH, KW)).astype(np.float32)
    bias_np = np.random.uniform(size=(1, CO, 1, 1)).astype(np.float32)
    conv_np = conv2d_nchw_python(data_np, weight_np, strides, padding)
    out_np = np.maximum(conv_np + bias_np, 0.0)

    dev = tvm.cuda()
    data_tvm = tvm.nd.array(data_np, device=dev)
    weight_tvm = tvm.nd.array(weight_np, device=dev)
    bias_tvm = tvm.nd.array(bias_np, device=dev)
    out_tvm = tvm.nd.empty(out_np.shape, device=dev)
    func(data_tvm, weight_tvm, bias_tvm, out_tvm)

    # Check results
    np.testing.assert_allclose(out_np, out_tvm.numpy(), rtol=1e-3)

    # Evaluate execution time
    evaluator = func.time_evaluator(func.entry_name, dev, min_repeat_ms=500)
    print(
        "Execution time of this operator: %.3f ms"
        % (np.median(evaluator(data_tvm, weight_tvm, bias_tvm, out_tvm).results) * 1000)
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Execution time of this operator: 0.383 ms




.. GENERATED FROM PYTHON SOURCE LINES 169-174

Using the record file
^^^^^^^^^^^^^^^^^^^^^
During the search, all measurement records are dumped into the record
file "conv2d.json". The measurement records can be used to re-apply search results,
resume the search, and perform other analyses.

.. GENERATED FROM PYTHON SOURCE LINES 176-179

Here is an example where we load the best schedule from a file,
print the equivalent python schedule API and CUDA source code.
They can be used for debugging and learning the behavior of the auto-scheduler.

.. GENERATED FROM PYTHON SOURCE LINES 179-186

.. code-block:: default


    print("Equivalent python schedule:")
    print(task.print_best(log_file, print_mode="schedule"))

    print("CUDA source code:")
    print(task.print_best(log_file, print_mode="cuda"))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Equivalent python schedule:
    pad_temp_i0, pad_temp_i1, pad_temp_i2, pad_temp_i3 = tuple(pad_temp.op.axis) + tuple(pad_temp.op.reduce_axis)
    conv2d_nchw_nn, conv2d_nchw_ff, conv2d_nchw_yy, conv2d_nchw_xx, conv2d_nchw_rc, conv2d_nchw_ry, conv2d_nchw_rx = tuple(conv2d_nchw.op.axis) + tuple(conv2d_nchw.op.reduce_axis)
    T_add_ax0, T_add_ax1, T_add_ax2, T_add_ax3 = tuple(T_add.op.axis) + tuple(T_add.op.reduce_axis)
    compute_i0, compute_i1, compute_i2, compute_i3 = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)
    s[T_add].compute_inline()
    conv2d_nchw_nn_o_i, conv2d_nchw_nn_i = s[conv2d_nchw].split(conv2d_nchw_nn, factor=1)
    conv2d_nchw_nn_o_o_i, conv2d_nchw_nn_o_i = s[conv2d_nchw].split(conv2d_nchw_nn_o_i, factor=1)
    conv2d_nchw_nn_o_o_o_i, conv2d_nchw_nn_o_o_i = s[conv2d_nchw].split(conv2d_nchw_nn_o_o_i, factor=1)
    conv2d_nchw_nn_o_o_o_o, conv2d_nchw_nn_o_o_o_i = s[conv2d_nchw].split(conv2d_nchw_nn_o_o_o_i, factor=1)
    conv2d_nchw_ff_o_i, conv2d_nchw_ff_i = s[conv2d_nchw].split(conv2d_nchw_ff, factor=2)
    conv2d_nchw_ff_o_o_i, conv2d_nchw_ff_o_i = s[conv2d_nchw].split(conv2d_nchw_ff_o_i, factor=1)
    conv2d_nchw_ff_o_o_o_i, conv2d_nchw_ff_o_o_i = s[conv2d_nchw].split(conv2d_nchw_ff_o_o_i, factor=16)
    conv2d_nchw_ff_o_o_o_o, conv2d_nchw_ff_o_o_o_i = s[conv2d_nchw].split(conv2d_nchw_ff_o_o_o_i, factor=1)
    conv2d_nchw_yy_o_i, conv2d_nchw_yy_i = s[conv2d_nchw].split(conv2d_nchw_yy, factor=1)
    conv2d_nchw_yy_o_o_i, conv2d_nchw_yy_o_i = s[conv2d_nchw].split(conv2d_nchw_yy_o_i, factor=1)
    conv2d_nchw_yy_o_o_o_i, conv2d_nchw_yy_o_o_i = s[conv2d_nchw].split(conv2d_nchw_yy_o_o_i, factor=7)
    conv2d_nchw_yy_o_o_o_o, conv2d_nchw_yy_o_o_o_i = s[conv2d_nchw].split(conv2d_nchw_yy_o_o_o_i, factor=1)
    conv2d_nchw_xx_o_i, conv2d_nchw_xx_i = s[conv2d_nchw].split(conv2d_nchw_xx, factor=1)
    conv2d_nchw_xx_o_o_i, conv2d_nchw_xx_o_i = s[conv2d_nchw].split(conv2d_nchw_xx_o_i, factor=1)
    conv2d_nchw_xx_o_o_o_i, conv2d_nchw_xx_o_o_i = s[conv2d_nchw].split(conv2d_nchw_xx_o_o_i, factor=7)
    conv2d_nchw_xx_o_o_o_o, conv2d_nchw_xx_o_o_o_i = s[conv2d_nchw].split(conv2d_nchw_xx_o_o_o_i, factor=1)
    conv2d_nchw_rc_o_i, conv2d_nchw_rc_i = s[conv2d_nchw].split(conv2d_nchw_rc, factor=8)
    conv2d_nchw_rc_o_o, conv2d_nchw_rc_o_i = s[conv2d_nchw].split(conv2d_nchw_rc_o_i, factor=1)
    conv2d_nchw_ry_o_i, conv2d_nchw_ry_i = s[conv2d_nchw].split(conv2d_nchw_ry, factor=1)
    conv2d_nchw_ry_o_o, conv2d_nchw_ry_o_i = s[conv2d_nchw].split(conv2d_nchw_ry_o_i, factor=3)
    conv2d_nchw_rx_o_i, conv2d_nchw_rx_i = s[conv2d_nchw].split(conv2d_nchw_rx, factor=3)
    conv2d_nchw_rx_o_o, conv2d_nchw_rx_o_i = s[conv2d_nchw].split(conv2d_nchw_rx_o_i, factor=1)
    s[conv2d_nchw].reorder(conv2d_nchw_nn_o_o_o_o, conv2d_nchw_ff_o_o_o_o, conv2d_nchw_yy_o_o_o_o, conv2d_nchw_xx_o_o_o_o, conv2d_nchw_nn_o_o_o_i, conv2d_nchw_ff_o_o_o_i, conv2d_nchw_yy_o_o_o_i, conv2d_nchw_xx_o_o_o_i, conv2d_nchw_nn_o_o_i, conv2d_nchw_ff_o_o_i, conv2d_nchw_yy_o_o_i, conv2d_nchw_xx_o_o_i, conv2d_nchw_rc_o_o, conv2d_nchw_ry_o_o, conv2d_nchw_rx_o_o, conv2d_nchw_rc_o_i, conv2d_nchw_ry_o_i, conv2d_nchw_rx_o_i, conv2d_nchw_nn_o_i, conv2d_nchw_ff_o_i, conv2d_nchw_yy_o_i, conv2d_nchw_xx_o_i, conv2d_nchw_rc_i, conv2d_nchw_ry_i, conv2d_nchw_rx_i, conv2d_nchw_nn_i, conv2d_nchw_ff_i, conv2d_nchw_yy_i, conv2d_nchw_xx_i)
    compute_i0_o_i, compute_i0_i = s[compute].split(compute_i0, factor=1)
    compute_i0_o_o_i, compute_i0_o_i = s[compute].split(compute_i0_o_i, factor=1)
    compute_i0_o_o_o, compute_i0_o_o_i = s[compute].split(compute_i0_o_o_i, factor=1)
    compute_i1_o_i, compute_i1_i = s[compute].split(compute_i1, factor=2)
    compute_i1_o_o_i, compute_i1_o_i = s[compute].split(compute_i1_o_i, factor=16)
    compute_i1_o_o_o, compute_i1_o_o_i = s[compute].split(compute_i1_o_o_i, factor=1)
    compute_i2_o_i, compute_i2_i = s[compute].split(compute_i2, factor=1)
    compute_i2_o_o_i, compute_i2_o_i = s[compute].split(compute_i2_o_i, factor=7)
    compute_i2_o_o_o, compute_i2_o_o_i = s[compute].split(compute_i2_o_o_i, factor=1)
    compute_i3_o_i, compute_i3_i = s[compute].split(compute_i3, factor=1)
    compute_i3_o_o_i, compute_i3_o_i = s[compute].split(compute_i3_o_i, factor=7)
    compute_i3_o_o_o, compute_i3_o_o_i = s[compute].split(compute_i3_o_o_i, factor=1)
    s[compute].reorder(compute_i0_o_o_o, compute_i1_o_o_o, compute_i2_o_o_o, compute_i3_o_o_o, compute_i0_o_o_i, compute_i1_o_o_i, compute_i2_o_o_i, compute_i3_o_o_i, compute_i0_o_i, compute_i1_o_i, compute_i2_o_i, compute_i3_o_i, compute_i0_i, compute_i1_i, compute_i2_i, compute_i3_i)
    s[conv2d_nchw].compute_at(s[compute], compute_i3_o_i)
    kernel_shared = s.cache_read(kernel, "shared", [conv2d_nchw])
    kernel_shared_ax0, kernel_shared_ax1, kernel_shared_ax2, kernel_shared_ax3 = tuple(kernel_shared.op.axis)
    s[kernel_shared].compute_at(s[conv2d_nchw], conv2d_nchw_rx_o_o)
    pad_temp_shared = s.cache_read(pad_temp, "shared", [conv2d_nchw])
    pad_temp_shared_ax0, pad_temp_shared_ax1, pad_temp_shared_ax2, pad_temp_shared_ax3 = tuple(pad_temp_shared.op.axis)
    s[pad_temp_shared].compute_at(s[conv2d_nchw], conv2d_nchw_rx_o_o)
    s[pad_temp].compute_inline()
    compute_i0_o_o_o_i1_o_o_o_fused_i2_o_o_o_fused_i3_o_o_o_fused = s[compute].fuse(compute_i0_o_o_o, compute_i1_o_o_o, compute_i2_o_o_o, compute_i3_o_o_o)
    s[compute].bind(compute_i0_o_o_o_i1_o_o_o_fused_i2_o_o_o_fused_i3_o_o_o_fused, te.thread_axis("blockIdx.x"))
    compute_i0_o_o_i_i1_o_o_i_fused_i2_o_o_i_fused_i3_o_o_i_fused = s[compute].fuse(compute_i0_o_o_i, compute_i1_o_o_i, compute_i2_o_o_i, compute_i3_o_o_i)
    s[compute].bind(compute_i0_o_o_i_i1_o_o_i_fused_i2_o_o_i_fused_i3_o_o_i_fused, te.thread_axis("vthread"))
    compute_i0_o_i_i1_o_i_fused_i2_o_i_fused_i3_o_i_fused = s[compute].fuse(compute_i0_o_i, compute_i1_o_i, compute_i2_o_i, compute_i3_o_i)
    s[compute].bind(compute_i0_o_i_i1_o_i_fused_i2_o_i_fused_i3_o_i_fused, te.thread_axis("threadIdx.x"))
    kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused = s[kernel_shared].fuse(kernel_shared_ax0, kernel_shared_ax1, kernel_shared_ax2, kernel_shared_ax3)
    kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o, kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_i = s[kernel_shared].split(kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused, factor=1)
    s[kernel_shared].vectorize(kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_i)
    kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o_o, kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o_i = s[kernel_shared].split(kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o, factor=784)
    s[kernel_shared].bind(kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o_i, te.thread_axis("threadIdx.x"))
    pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused = s[pad_temp_shared].fuse(pad_temp_shared_ax0, pad_temp_shared_ax1, pad_temp_shared_ax2, pad_temp_shared_ax3)
    pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o, pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_i = s[pad_temp_shared].split(pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused, factor=1)
    s[pad_temp_shared].vectorize(pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_i)
    pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o_o, pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o_i = s[pad_temp_shared].split(pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o, factor=784)
    s[pad_temp_shared].bind(pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o_i, te.thread_axis("threadIdx.x"))
    s[conv2d_nchw].pragma(conv2d_nchw_nn_o_o_o_o, "auto_unroll_max_step", 512)
    s[conv2d_nchw].pragma(conv2d_nchw_nn_o_o_o_o, "unroll_explicit", True)

    CUDA source code:

    #ifdef _WIN32
      using uint = unsigned int;
      using uchar = unsigned char;
      using ushort = unsigned short;
      using int64_t = long long;
      using uint64_t = unsigned long long;
    #else
      #define uint unsigned int
      #define uchar unsigned char
      #define ushort unsigned short
      #define int64_t long long
      #define uint64_t unsigned long long
    #endif
    extern "C" __global__ void __launch_bounds__(784) default_function_kernel0(float* __restrict__ data, float* __restrict__ kernel, float* __restrict__ compute, float* __restrict__ bias) {
      float conv2d_nchw[2];
      __shared__ float pad_temp_shared[648];
      __shared__ float kernel_shared[2304];
      conv2d_nchw[0] = 0.000000e+00f;
      conv2d_nchw[1] = 0.000000e+00f;
      for (int rc_outer_outer = 0; rc_outer_outer < 64; ++rc_outer_outer) {
        __syncthreads();
        if (((int)threadIdx.x) < 648) {
          pad_temp_shared[((int)threadIdx.x)] = (((((9 <= (((int)threadIdx.x) % 81)) && ((((int)threadIdx.x) % 81) < 72)) && (1 <= (((int)threadIdx.x) % 9))) && ((((int)threadIdx.x) % 9) < 8)) ? data[(((((rc_outer_outer * 392) + ((((int)threadIdx.x) / 81) * 49)) + (((((int)threadIdx.x) % 81) / 9) * 7)) + (((int)threadIdx.x) % 9)) - 8)] : 0.000000e+00f);
        }
        kernel_shared[((int)threadIdx.x)] = kernel[((((((int)blockIdx.x) * 147456) + ((((int)threadIdx.x) / 72) * 4608)) + (rc_outer_outer * 72)) + (((int)threadIdx.x) % 72))];
        kernel_shared[(((int)threadIdx.x) + 784)] = kernel[(((((((int)blockIdx.x) * 147456) + (((((int)threadIdx.x) + 784) / 72) * 4608)) + (rc_outer_outer * 72)) + ((((((int)threadIdx.x) + 64) % 72) / 3) * 3)) + ((((int)threadIdx.x) + 1) % 3))];
        if (((int)threadIdx.x) < 736) {
          kernel_shared[(((int)threadIdx.x) + 1568)] = kernel[(((((((int)blockIdx.x) * 147456) + (((((int)threadIdx.x) + 1568) / 72) * 4608)) + (rc_outer_outer * 72)) + ((((((int)threadIdx.x) + 56) % 72) / 3) * 3)) + ((((int)threadIdx.x) + 2) % 3))];
        }
        __syncthreads();
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7))] * kernel_shared[((((int)threadIdx.x) / 49) * 144)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7))] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 72)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 1)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 1)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 1)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 73)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 2)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 2)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 2)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 74)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 81)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 9)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 81)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 81)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 82)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 10)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 82)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 82)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 83)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 11)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 83)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 83)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 162)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 18)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 162)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 90)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 163)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 19)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 163)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 91)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 164)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 20)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 164)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 92)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 243)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 27)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 243)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 99)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 244)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 28)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 244)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 100)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 245)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 29)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 245)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 101)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 324)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 36)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 324)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 108)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 325)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 37)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 325)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 109)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 326)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 38)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 326)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 110)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 405)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 45)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 405)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 117)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 406)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 46)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 406)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 118)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 407)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 47)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 407)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 119)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 486)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 54)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 486)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 126)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 487)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 55)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 487)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 127)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 488)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 56)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 488)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 128)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 567)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 63)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 567)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 135)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 568)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 64)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 568)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 136)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 569)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 65)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 569)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 137)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 9)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 3)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 9)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 75)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 10)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 4)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 10)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 76)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 11)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 5)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 11)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 77)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 90)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 12)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 90)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 84)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 91)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 13)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 91)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 85)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 92)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 14)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 92)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 86)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 171)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 21)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 171)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 93)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 172)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 22)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 172)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 94)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 173)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 23)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 173)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 95)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 252)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 30)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 252)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 102)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 253)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 31)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 253)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 103)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 254)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 32)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 254)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 104)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 333)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 39)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 333)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 111)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 334)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 40)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 334)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 112)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 335)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 41)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 335)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 113)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 414)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 48)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 414)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 120)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 415)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 49)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 415)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 121)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 416)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 50)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 416)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 122)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 495)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 57)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 495)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 129)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 496)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 58)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 496)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 130)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 497)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 59)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 497)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 131)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 576)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 66)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 576)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 138)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 577)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 67)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 577)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 139)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 578)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 68)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 578)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 140)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 18)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 6)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 18)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 78)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 19)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 7)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 19)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 79)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 20)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 8)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 20)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 80)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 99)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 15)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 99)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 87)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 100)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 16)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 100)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 88)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 101)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 17)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 101)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 89)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 180)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 24)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 180)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 96)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 181)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 25)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 181)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 97)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 182)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 26)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 182)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 98)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 261)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 33)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 261)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 105)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 262)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 34)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 262)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 106)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 263)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 35)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 263)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 107)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 342)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 42)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 342)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 114)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 343)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 43)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 343)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 115)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 344)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 44)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 344)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 116)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 423)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 51)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 423)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 123)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 424)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 52)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 424)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 124)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 425)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 53)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 425)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 125)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 504)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 60)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 504)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 132)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 505)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 61)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 505)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 133)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 506)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 62)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 506)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 134)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 585)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 69)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 585)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 141)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 586)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 70)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 586)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 142)]));
        conv2d_nchw[0] = (conv2d_nchw[0] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 587)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 71)]));
        conv2d_nchw[1] = (conv2d_nchw[1] + (pad_temp_shared[(((((((int)threadIdx.x) % 49) / 7) * 9) + (((int)threadIdx.x) % 7)) + 587)] * kernel_shared[(((((int)threadIdx.x) / 49) * 144) + 143)]));
      }
      for (int i1_inner = 0; i1_inner < 2; ++i1_inner) {
        compute[((((((int)blockIdx.x) * 1568) + ((((int)threadIdx.x) / 49) * 98)) + (i1_inner * 49)) + (((int)threadIdx.x) % 49))] = max((conv2d_nchw[i1_inner] + bias[(((((int)blockIdx.x) * 32) + ((((int)threadIdx.x) / 49) * 2)) + i1_inner)]), 0.000000e+00f);
      }
    }






.. GENERATED FROM PYTHON SOURCE LINES 187-191

A more complicated example is to resume the search.
In this case, we need to create the search policy and cost model by ourselves
and resume the status of search policy and cost model with the log file.
In the example below we resume the status and do more 5 trials.

.. GENERATED FROM PYTHON SOURCE LINES 191-213

.. code-block:: default



    def resume_search(task, log_file):
        print("Resume search:")
        cost_model = auto_scheduler.XGBModel()
        cost_model.update_from_file(log_file)
        search_policy = auto_scheduler.SketchPolicy(
            task, cost_model, init_search_callbacks=[auto_scheduler.PreloadMeasuredStates(log_file)]
        )
        measure_ctx = auto_scheduler.LocalRPCMeasureContext(min_repeat_ms=300)
        tune_option = auto_scheduler.TuningOptions(
            num_measure_trials=5,
            runner=measure_ctx.runner,
            measure_callbacks=[auto_scheduler.RecordToFile(log_file)],
        )
        task.tune(tune_option, search_policy=search_policy)

        # Kill the measurement process
        del measure_ctx


    resume_search(task, log_file)




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Resume search:
    /venv/apache-tvm-py3.7/lib/python3.7/site-packages/xgboost/training.py:17: UserWarning: Old style callback is deprecated.  See: https://xgboost.readthedocs.io/en/latest/python/callbacks.html
      warnings.warn(f'Old style callback is deprecated.  See: {link}', UserWarning)
    Get devices for measurement successfully!






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 5 minutes  59.876 seconds)


.. _sphx_glr_download_how_to_tune_with_autoscheduler_tune_conv2d_layer_cuda.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: tune_conv2d_layer_cuda.py <tune_conv2d_layer_cuda.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: tune_conv2d_layer_cuda.ipynb <tune_conv2d_layer_cuda.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
